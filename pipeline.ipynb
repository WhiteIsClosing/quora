{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from functools import partial\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from collections import defaultdict\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import PolynomialFeatures, scale, FunctionTransformer\n",
    "from nltk import stem\n",
    "from sner import Ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "non_ascii = re.compile(r'[^A-Za-z0-9 ]')\n",
    "countries = ['usa', 'china', 'europe', 'united states', 'america', 'mexico', 'india', 'hindi',\n",
    "            'chinese', 'european', 'american', 'mexican', 'spain', 'italy', 'russia', 'italian',\n",
    "            'great britain', 'british']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ngram_features(row, n_gram=1):\n",
    "    \n",
    "    counts = []\n",
    "    if isinstance(n_gram, int):\n",
    "        n_grams = [n_gram]\n",
    "    del n_gram\n",
    "    for n_gram in n_grams:\n",
    "        ques = list(map(lambda x: x.split(), row[['question1', 'question2']].tolist()))\n",
    "        first = [' '.join(ques[0][j:j+n_gram]) for j in range(len(ques[0]) - n_gram +1)]\n",
    "        second = [' '.join(ques[1][j:j+n_gram]) for j in range(len(ques[1])- n_gram +1)]\n",
    "\n",
    "        sfirst = set(first)\n",
    "        ssecond = set(second)\n",
    "        common = [x for x in first if x in ssecond] + [x for x in second if x in sfirst]\n",
    "\n",
    "        arr = [len(common), len(first), len(second), len(sfirst), len(ssecond)]\n",
    "\n",
    "        ag1 = sum(arr[1:3])\n",
    "        ag2 = sum(arr[3:])\n",
    "        z = []\n",
    "        if ag1 == 0:\n",
    "            z.append(0)\n",
    "        else:\n",
    "            z.append(arr[0]/ag1)\n",
    "\n",
    "        if ag2 == 0:\n",
    "            z.append(0)\n",
    "        else:\n",
    "            z.append(arr[0]/ag2)\n",
    "\n",
    "        z.append( abs( arr[3] - arr[4] ) )\n",
    "        z.append( abs( arr[1] - arr[2] ) )\n",
    "        \n",
    "        counts += z\n",
    "        \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_subset(row):\n",
    "    \n",
    "    words_1 = set(row['question1'].lower().split())\n",
    "    words_2 = set(row['question2'].lower().split())\n",
    "    \n",
    "    if len(words_1) - len(words_2) == len(words_1 - words_2):\n",
    "        \n",
    "        return 1\n",
    "    \n",
    "    elif len(words_2) - len(words_1) == len(words_2 - words_1):\n",
    "        \n",
    "        return -1\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_score_match(row, vect):\n",
    "    \n",
    "    q1 = vect.transform([row['question1']])\n",
    "    q2 = vect.transform([row['question2']])\n",
    "    \n",
    "    nonzero1 = set(q1.nonzero()[1])\n",
    "    nonzero2 = set(q2.nonzero()[1])\n",
    "    \n",
    "    numerator = q1[0, list(nonzero2)].sum() + q2[0, list(nonzero1)].sum()\n",
    "    denominator = q1.sum() + q2.sum()\n",
    "    if denominator:\n",
    "        return numerator/denominator\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lengths_info(row, stop_words=[]):\n",
    "    \n",
    "    if len(stop_words):\n",
    "        \n",
    "        row['question1'] = [x for x in row['question1'].split() if x not in stop_words]\n",
    "        row['question2'] = [x for x in row['question2'].split() if x not in stop_words]\n",
    "    \n",
    "    l1 = len(row['question1'])\n",
    "    l2 = len(row['question2'])\n",
    "    \n",
    "    if l1 == 0 or l2 == 0:\n",
    "        return [0]*5\n",
    "    \n",
    "    word_length_1 = list(map(len, row['question1']))\n",
    "    word_length_2 = list(map(len, row['question2']))\n",
    "    \n",
    "    avg_word_length_1 = np.mean(word_length_1) if len(word_length_1) else 0\n",
    "    avg_word_length_2 = np.mean(word_length_2) if len(word_length_2) else 0\n",
    "    \n",
    "    median_wl_1 = np.median(word_length_1) if len(word_length_1) else 0\n",
    "    median_wl_2 = np.median(word_length_2) if len(word_length_2) else 0\n",
    "    \n",
    "    uniq_l1, uniq_cnt_l1 = np.unique(word_length_1, return_counts=True)\n",
    "    uniq_l2, uniq_cnt_l2 = np.unique(word_length_2, return_counts=True)\n",
    "    \n",
    "    var_word_length_1 = np.var(word_length_1) if len(word_length_1) else 0\n",
    "    var_word_length_2 = np.var(word_length_2) if len(word_length_2) else 0\n",
    "    \n",
    "    return [abs(avg_word_length_1-avg_word_length_2), abs(median_wl_1-median_wl_2),\n",
    "            abs(var_word_length_1-var_word_length_2), abs(l1-l2),\n",
    "            abs(np.log(l1)-np.log(l2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_vect_cosine(row, vect):\n",
    "    \n",
    "    q1 = vect.transform([row['question1']])\n",
    "    q2 = vect.transform([row['question2']])\n",
    "\n",
    "    return q1.dot(q2.T).toarray().flatten()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('preprocessed_to_word2vec.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RowTransformer(TransformerMixin):\n",
    "    \n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "        \n",
    "    def translate(self, X):\n",
    "        d = X.apply(self.func, axis=1).as_matrix()\n",
    "        try:\n",
    "            s = len(d[0])\n",
    "        except:\n",
    "            s = 1\n",
    "        z = np.zeros((d.shape[0], s))\n",
    "        for i in range(d.shape[0]):\n",
    "            z[i, :] = d[i]\n",
    "        return z\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.translate(X)\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        return self.translate(X)\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"President Obama took his daughter into White House.\"\n",
    "tagger = Ner(host='localhost',port=9199)\n",
    "\n",
    "\n",
    "def get_entities(row):\n",
    "    \n",
    "    q1 = [x for x in tagger.get_entities(row['question1']) if x[1] != 'O']\n",
    "    q2 = [x for x in tagger.get_entities(row['question2']) if x[1] != 'O']\n",
    "    \n",
    "    return [q1, q2]\n",
    "\n",
    "ent = raw_train.ix[:10000].apply(get_entities, axis=1)\n",
    "\n",
    "ent.apply(lambda x: True if x == [[], []] else False).mean()\n",
    "\n",
    "types_of_entities = set()\n",
    "\n",
    "for row in ent:\n",
    "    \n",
    "    types_of_entities = types_of_entities | set([x[1] for x in row[1]]) | set([x[1] for x in row[0]])\n",
    "\n",
    "types_of_entities\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_entities_diff(row):\n",
    "    \n",
    "    counts = [defaultdict(list), defaultdict(list)]\n",
    "    \n",
    "    for r in range(2):\n",
    "        \n",
    "        for ent in row[r]:\n",
    "            \n",
    "            counts[r][ent[1]].append(ent[0])\n",
    "            \n",
    "    stats = []\n",
    "            \n",
    "    for t in ['LOCATION', 'ORGANIZATION', 'PERSON']:\n",
    "        \n",
    "        common = [x for x in counts[0][t] if x in counts[1][t]] + [x for x in counts[1][t] if x in counts[0][t]]\n",
    "        logor = counts[0][t] + counts[1][t]\n",
    "        prop = len(common)/len(logor) if len(logor) else 0\n",
    "        \n",
    "        stats += [prop, len(common), len(logor), len(counts[1][t]), len(counts[0][t]), \n",
    "                  abs(len(counts[1][t]) - len(counts[0][t]))]\n",
    "        \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_to_vectorize = np.concatenate([train['question1'], train['question2']], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=1024, stop_words='english', ngram_range=(1,3)).fit(base_to_vectorize)\n",
    "count = CountVectorizer(max_features=1024, stop_words='english', ngram_range=(1,3)).fit(base_to_vectorize)\n",
    "binary = CountVectorizer(binary=True, max_features=1024, stop_words='english', ngram_range=(1,3)).fit(base_to_vectorize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_match = partial(get_score_match, vect=tfidf)\n",
    "count_match = partial(get_score_match, vect=count)\n",
    "binary_match = partial(get_score_match, vect=binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.910801\n",
       "1    0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.ix[:1].apply(tfidf_match, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_cosine = partial(calculate_vect_cosine, vect=tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grand_transformer = make_union(*[RowTransformer(x) for x in [tfidf_cosine, tfidf_match, count_match, binary_match,\n",
    "                                                             is_subset, get_lengths_info, get_ngram_features]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tent = grand_transformer.transform(train.ix[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.57981578889201402"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=5)\n",
    "cross_val_score(lr, tent, train.ix[:len(tent)-1, 'is_duplicate'], scoring='neg_log_loss', cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "entx = raw_train.ix[:10000].apply(get_entities, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entx = np.asarray(entx.apply(get_entities_diff).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.568670262818\n",
      "-0.568214044144\n",
      "-0.567968685893\n",
      "-0.567793623461\n",
      "-0.567668578216\n",
      "-0.567632151996\n",
      "-0.567577609\n",
      "-0.567581787563\n",
      "-0.567492825674\n",
      "-0.567484730666\n"
     ]
    }
   ],
   "source": [
    "for C in range(1, 11):\n",
    "    lr = LogisticRegression(C=C)\n",
    "    print(\n",
    "        cross_val_score(lr, np.hstack([tent, entx]), train.ix[:len(tent)-1, 'is_duplicate'], \n",
    "                        scoring='neg_log_loss', cv=10).mean()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10001, 14)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tent.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
